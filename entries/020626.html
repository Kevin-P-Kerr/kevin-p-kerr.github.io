<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How (Not) To Talk About LLM's</title>
    <link rel="stylesheet" href="../blog_styles.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=AM_CHTML"></script>
</head>
<body>
    <a href="../index.html"> home </a> <br>
    <div class="container">
        <h1>How (Not) To Talk About LLM's</h1>
        <H3>They're "Just" Next-Token Predictors</H3>
        Consider this, from a <a href="https://s10251.pcdn.co/pdf/2021-bender-parrots.pdf"> widely cited paper</a> on the dangers of large language models. <br><blockquote> Contrary to how it may seem when we observe its output, an LM is a system
for haphazardly stitching together sequences of linguistic forms
it has observed in its vast training data, according to probabilistic
information about how they combine, but without any reference to
meaning: a stochastic parrot. </blockquote> <br>
Or,<a href="https://arxiv.org/pdf/2212.03551"> again</a> <br><blockquote> LLMs are generative mathematical models of the statistical distribution
of tokens in the vast public corpus of human generated text, where the tokens in question include words, parts of words, or individual characters including punctuation marks. They are generative because we can sample from them, which means we can ask them questions. But the questions are of the following very specific kind. “Here’s a fragment of text. Tell me how this fragment might go on. According to your model of the statistics of human language, what words are likely to come next?” </blockquote><br>
Now, the first quote combines two claims in very short order--one about the limitations of LLMs based on their underlying mechanism, namely that they "stitch together linguistic forms...according to probabilistic information", the other claiming that they act "without any reference to meaning", on which more anon. The second quote is much more straightforward: the statistical mechanisms which underlie LLMs limit us to asking very narrow questions about which text is most likely to follow from a certain input. <br>
The argument seems to be thus<br><blockquote> 
<p> i. "merely statistical" means of computing next token predictions are limited in the types of queries they can answer</p>
<p>ii. LLMs are "mereley statistical" means of predicting the next token in some text </p> 
<p>conclusion: LLMs' are limited in the types of queries they can answer.</p></blockquote>
Where this goes wrong is in the major premise: as is now well known, even relatively simple autoregressive next-token predictors can be trained to emulate any efficiently turing computable function.  And as many programmers have learned from bitter experience, if you've "just" implemented a turing machine, you've actually done quite a lot. In fact, chances are you have done <a href="https://stackoverflow.com/a/1732454"> too much </a><br>
I believe it's in failing to sufficiently appreciate this point that leads the author of the second paper into this rather confused digression:
<blockquote>
One tempting line of argument goes like this.
Although large language models, at root, only
perform sequence prediction, it’s possible that,
in learning to do this, they have discovered
emergent mechanisms that warrant a description
in higher-level terms. These higher-level terms
might include “knowledge” and “belief”. Indeed,
we know that artificial neural networks can approximate any computable function to an arbitrary degree of accuracy. So, given enough parameters, data, and computing power, perhaps
stochastic gradient descent will discover such
mechanisms if they are the best way to optimise
the objective of making accurate sequence predictions.
Again, it’s important here to distinguish between the bare-bones model and the whole system. Only in the context of a capacity to distinguish truth from falsehood can we legitimately
speak of “belief” in its fullest sense. But an
LLM — the bare-bones model — is not in the
business of making judgements. It just models
what words are likely to follow from what other
words. </blockquote>
Now, leaving to one side (to which we will shortly return) the extent to which terms like "knowledge" and "belief" are "mechanisms" that a machine must instantiate or "discover" in order to have intelligence, it is surely very strange to note that, on the one hand, autogressive next token predictors like LLM's can emulate a universal model of computation, and then on the other, claim they "just" model next token prediction. The word "just" is doing a lot of work there! It's telling that I can rewrite the following quote as 
<blockquote> But a turing machine--the bare-bones model--is not in the business of making judgements. It just outputs the next token deterministically, given the current input and its current state</blockquote> 
One is left asking if the author would concede any condition underwhich a mechanical computing procedure would count as "intelligent"
<H2> They Don't Do What Humans Do </H2>
As we saw above, the discussion about "just being a next token predictor" quickly segue from the theoretical analysis of different type of computation, to something more nebulous, that applies to any mechanical process more generally.  Purely mechanical processes cannot "distinguish truth from falsehood" so they cannot have "belief"; they do not "take intentional stances", so their output lacks "meaning", without which it is nothing the but the illusion of intelligent behavior. As an author of the first paper cited <a href="https://aclanthology.org/2020.acl-main.463/"> puts it </a> </br>
<blockquote>
 The speaker has a certain communicative intent i, and chooses an expression e with a
standing meaning s which is fit to express i in the
current communicative situation. Upon hearing e,
the listener then reconstructs s and uses their own
knowledge of the communicative situation and their
hypotheses about the speaker’s state of mind and
intention in an attempt to deduce i.</blockquote> <br>
But all of this is dubious. The authors says the speaker "has a certain communicative intent", but where is that? Presumably somewhere "inside" her head. But then how is it accessible to observation? And again the speaker supposedly "chooses an expression", "reconstructs [meaning]", "hypothesizes about [others'] state of mind", and so on.  But what evidence is there that that is "what is going on inside the head" of a any particular person when speaking? Perhaps the author would respond that this is intuitively obvious, or that this information is somehow available to us through introspection. But many intuitively obvious things just aren't so. For example, to go back to "choosing an expression", it's not clear at all in what this choosing consists.  Choose for me, if you will, the first song that comes to your head. Why did you "choose" it? And in what sense did you choose it? As Jon Hershfield and Blaise Aguirre note
<blockquote>
Why did you not pick a different song? Take a moment to reflect... You may have what seems like a reasonable answer for that question, but the reality is that the other song did not show up when you looked for a song. In other words, the thought chose you more than you chose the thought.</blockquote> 
One wishes the authors had better internalized Skinner's <a href="https://www.jstor.org/stable/27758892">late career meditation</a>
<blockquote>
Take, for example, the so-called process of association. In Pavlov's experiment a
hungry dog hears a bell and is then fed. If this happens many times, the dog begins to
salivate when it hears the bell. The standard mentalistic explanation is that the dog
"associates" the bell with the food. But it was Pavlov who associated them!..."Word Associations" are at least correctly named. If we say "home" when someone says "house," it is not because we associate the two words but because they are associated in daily English usage. Cognitive association is an invention. Even if it were real, it would go no further towards an explanation than the external contigencies upon which it is based" </blockquote>
All of which to say is that it is a very fraught bussiness indeed to try to infer what must be happening in our heads when we speak or do any other type of cognitively demanding task. Nor does it matter when assessing the capabilities of a machine to emulate our intelligent behavior. What does matter is (a) that behavior itself and (b) the theoretical underpinnings that dictate what the machine is capable of computing. 
The authors go on to formulate a thought experiment: <blockquote>
Say that A and B, both fluent
speakers of English, are independently stranded on two uninhabited islands. They soon discover that
previous visitors to these islands have left behind
telegraphs and that they can communicate with
each other via an underwater cable. A and B start
happily typing messages to each other.
Meanwhile, O, a hyper-intelligent deep-sea octopus who is unable to visit or observe the two
islands, discovers a way to tap into the underwater cable and listen in on A and B’s conversations.
O knows nothing about English initially, but is
very good at detecting statistical patterns. Over
time, O learns to predict with great accuracy how
B will respond to each of A’s utterances. O also
observes that certain words tend to occur in similar
contexts, and perhaps learns to generalize across
lexical patterns by hypothesizing that they can be
used somewhat interchangeably. Nonetheless, O
has never observed these objects, and thus would
not be able to pick out the referent of a word when
presented with a set of (physical) alternatives.
At some point, O starts feeling lonely. He cuts
the underwater cable and inserts himself into the
conversation, by pretending to be B and replying
to A’s messages....Finally, A faces an emergency. She is suddenly
pursued by an angry bear. She grabs a couple of
sticks and frantically asks B to come up with a way
to construct a weapon to defend herself. Of course,
O has no idea what A “means”. Solving a task like
this requires the ability to map accurately between
words and real-world entities (as well as reasoning
and creative thinking). It is at this point that O
would fail the Turing test </blockquote>

The argument seems to be that the squid could not accurately or usefully respond to novel situations. Perhaps the squid, then, was not so intelligent. But if it were intelligent, than surely there is no problem here. It is surely possible to compute "what to do when attacked by bear given some coconuts" and "what to do when encountering a new word".  Whatever it means to "map accurately between words and real-world entities", it is not necessary to solve this problem. 

So where does this leave us? Must we be cheer-leaders for big tech and big ai in particular?  Certainly not. But I can question if instagram is good for teenagers or for the world more broadly without questioning if it's actually an application that connects tens of millions of users. Perhaps the big ai players do not yet have sufficient training data or compute or the right architecture or the right understanding of the target function to effeciently learn human intelligence. But to say ahead of time based on dubious philosophizing that such a task is not possible doesn't help matters.
